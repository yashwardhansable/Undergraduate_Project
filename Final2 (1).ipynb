{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d84bd96",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3dde211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import readsav\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267efad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sunpy\n",
    "import sunpy.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b1f9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471e6129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2d8d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca547e1",
   "metadata": {},
   "source": [
    "# Useful General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b6cb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Test\\nim = get_file_paths(\"G:\\\\BMR_Identification\")\\nimg = image_data_generator(im)\\nimage = next(img)\\n\\nplt.figure(figsize=(10, 10))\\nplt.imshow(image, cmap=\\'gray\\', origin=\\'lower\\')\\nplt.colorbar()\\nplt.title(\\'Image Data from IDL .sav file\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_paths(directory_path):\n",
    "    \"\"\"\n",
    "    Given a directory path, this function returns a list of full file paths\n",
    "    for all the files within the given directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path (str): The path to the directory from which to list files.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of full file paths.\n",
    "    \"\"\"\n",
    "    # List to hold file paths\n",
    "    file_paths = []\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.isdir(directory_path):\n",
    "        raise ValueError(f\"The provided directory path does not exist: {directory_path}\")\n",
    "\n",
    "    # Walk the directory tree\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            # Concatenate the root directory and file name to get the full path\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\"\"\"\n",
    "# Test\n",
    "im = get_file_paths(\"G:\\\\BMR_Identification\")\n",
    "img = image_data_generator(im)\n",
    "image = next(img)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image, cmap='gray', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('Image Data from IDL .sav file')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc75eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_generator(x_generator, y_generator, batch_size):\n",
    "    while True:  # Loop indefinitely\n",
    "        X_batch = []\n",
    "        Y_batch = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            X_batch.append(next(x_generator))\n",
    "            Y_batch.append(next(y_generator))\n",
    "        \n",
    "        # Convert to numpy arrays and yield the batches\n",
    "        yield (np.array(X_batch), np.array(Y_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a574b49",
   "metadata": {},
   "source": [
    "# Sun Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1364430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fits_image_sunpy(fits_file_path, vmin=-1500, vmax=1500):\n",
    "    # Use sunpy to open the FITS file\n",
    "    magnetogram_map = sunpy.map.Map(fits_file_path)\n",
    "    image_data = magnetogram_map.data.astype(np.float32)  # Ensure data is in float32\n",
    "    \n",
    "    # Clip the data to be within the range [vmin, vmax]\n",
    "    image_data = np.clip(image_data, vmin, vmax)\n",
    "    \n",
    "    # Normalize the clipped image data to [0, 1]\n",
    "    image_data = (image_data - vmin) / (vmax - vmin)\n",
    "    \n",
    "    image_data = np.nan_to_num(image_data, nan=0)\n",
    "    return image_data\n",
    "\n",
    "def sun_data_generator(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        yield load_fits_image_sunpy(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff44da",
   "metadata": {},
   "source": [
    "# BMR Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43276d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sav_file(sav_file_path):\n",
    "    # Read the .sav file\n",
    "    idl_data = readsav(sav_file_path)\n",
    "    \n",
    "    # Extract image dimensions and indices\n",
    "    hdr_los = idl_data['hdr_los']\n",
    "    naxis1 = hdr_los.naxis1[0]\n",
    "    naxis2 = hdr_los.naxis2[0]\n",
    "    bmr_ind = idl_data['bmr_ind']\n",
    "    \n",
    "    # Convert flat indices to 2D indices\n",
    "    rows, cols = np.divmod(bmr_ind, naxis1)\n",
    "\n",
    "    # Create an empty image and set the magnetic regions to 1\n",
    "    image_data = np.zeros((naxis2, naxis1), dtype=np.float32)\n",
    "    image_data[rows, cols] = 1.0\n",
    "    \n",
    "    image_data = np.expand_dims(image_data, axis=-1)\n",
    "\n",
    "    return image_data\n",
    "\n",
    "def image_data_generator(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        yield process_sav_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3451173",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d51c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, Lambda, Cropping2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_small_unet(input_size=(1024, 1024, 1), pretrained_weights=None):\n",
    "    inputs = Input(input_size)\n",
    "    x = Lambda(lambda x: tf.tile(x, multiples=[1, 1, 1, 3]))(inputs)  # Convert to RGB\n",
    "\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=x, input_shape=(1024, 1024, 3))\n",
    "\n",
    "    # Freeze the layers of MobileNetV2\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define the layer names for skip connections\n",
    "    layer_names = [\n",
    "        \"block_1_expand_relu\",   # downsampled to 512x512\n",
    "        \"block_3_expand_relu\",   # downsampled to 256x256\n",
    "        \"block_6_expand_relu\",   # downsampled to 128x128\n",
    "        # Add the additional layers needed for skip connections if you want more upsampling steps\n",
    "    ]\n",
    "    layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Create the decoder\n",
    "    x = base_model.output\n",
    "    # Start the upsampling process to match the downsampled layers\n",
    "    for filters, skip_layer in zip([512, 256, 128, 64], layers[::-1]):  # Adjust the number of filters as needed\n",
    "        x = Conv2DTranspose(filters, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "        # Calculate cropping for skip layer\n",
    "        skip_size = skip_layer.shape[1:3]\n",
    "        x_size = x.shape[1:3]\n",
    "        crop_size = [(s - x_s) // 2 for s, x_s in zip(skip_size, x_size)]\n",
    "        cropped_skip_layer = Cropping2D(cropping=(crop_size, crop_size))(skip_layer)\n",
    "        x = concatenate([x, cropped_skip_layer])\n",
    "        x = Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # Additional upsampling steps to get back to the original size\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(x)  # Upsample to 512x512\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(x)  # Upsample to 1024x1024\n",
    "\n",
    "    # Output layer for binary segmentation\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    if pretrained_weights:\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfd039",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41f323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a913bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "684d082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(X_train_file_path, Y_train_file_path, X_validation_file_path, Y_validation_file_path, Yash_path):\n",
    "    \n",
    "    # Create the U-Net model\n",
    "    model = create_small_unet()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    list_of_x_train_files = get_file_paths(X_train_file_path)\n",
    "    list_of_y_train_files = get_file_paths(Y_train_file_path)\n",
    "    x_train_generator = sun_data_generator(list_of_x_train_files)\n",
    "    y_train_generator = image_data_generator(list_of_y_train_files)\n",
    "\n",
    "    # Combine them into a single generator for training\n",
    "    train_generator = combined_generator(x_train_generator, y_train_generator, batch_size=8)\n",
    "\n",
    "    # Calculate the steps per epoch (total_samples / batch_size)\n",
    "    steps_per_epoch = len(list_of_x_train_files) // 8\n",
    "\n",
    "    # Prepare your generators for validation\n",
    "    list_of_x_test_files = get_file_paths(X_validation_file_path)\n",
    "    list_of_y_test_files = get_file_paths(Y_validation_file_path)\n",
    "    x_test_generator = sun_data_generator(list_of_x_test_files)\n",
    "    y_test_generator = image_data_generator(list_of_y_test_files)\n",
    "\n",
    "    # Combine them into a single generator for validation\n",
    "    validation_generator = combined_generator(x_test_generator, y_test_generator, batch_size=8)\n",
    "\n",
    "    # Define validation steps if you haven't\n",
    "    validation_steps = len(list_of_x_test_files) // 8\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    verbose=1, \n",
    "    mode='min'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=10,  # Replace with the number of epochs you desire\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks = [early_stopping]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    file_name = 'my_plot.png'\n",
    "    \n",
    "    full_path = os.path.join(Yash_path, file_name)\n",
    "    \n",
    "    plt.savefig(full_path)\n",
    "\n",
    "    # Save the model after training\n",
    "    model.save(os.path.join(Yash_path, my_model.h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bb0883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9434a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
